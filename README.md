# seeing-eye-dog
learning to use mlx by 
1. doing vlm inference with quantized models.
2. deploy on device for (hopefully) realtime inference
3. make it better idk
